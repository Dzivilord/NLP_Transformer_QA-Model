{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11571430,"sourceType":"datasetVersion","datasetId":7254572}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ======= Install & Setup =======\n!pip install -q transformers\n\nimport os\nimport gc\nimport re\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForQuestionAnswering,\n    get_scheduler\n)\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import GradScaler, autocast\nimport multiprocessing\nimport random\n\n# ======= Environment Setup =======\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\n# bfloat16 optimization (if supported)\ntorch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = True\n\n# ======= Config =======\ntrain_path               = \"/kaggle/input/cleaned/trained_dataset.json\"\nvalid_path               = \"/kaggle/input/cleaned/valid_dataset.json\"\nmodel_name               = \"deepset/roberta-base-squad2\"\ntrain_batch_size         = 16\neval_batch_size          = 80\nmax_length               = 512\nstride                   = 300\naccumulation_steps       = 10\nepochs                   = 1\nearly_stopping_patience  = 1\nlr                       = 5e-5\nnum_proc                 = min(10, multiprocessing.cpu_count())\nseed                     = 42\n\n# ======= Output dirs =======\noutput_dir        = \"/kaggle/working/\"\nmodel_save_dir    = os.path.join(output_dir, \"best_model\")\ncheckpoint_dir    = os.path.join(output_dir, \"checkpoints\")\nos.makedirs(model_save_dir, exist_ok=True)\nos.makedirs(checkpoint_dir, exist_ok=True)\n\nresume_checkpoint = \"/kaggle/working/checkpoints/checkpoint-epoch1\"\n\n# ======= Load dataset =======\nraw_datasets = load_dataset(\"json\", data_files={\n    \"train\": train_path,\n    \"validation\": valid_path\n})\ntrain_dataset = raw_datasets[\"train\"]\nvalid_dataset = raw_datasets[\"validation\"]\n\n# ======= Tokenizer & Model =======\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\n    print(f\"Using {torch.cuda.device_count()} GPUs\")\n\n# ======= Resume checkpoint =======\nif resume_checkpoint:\n    ckpt_path = os.path.join(checkpoint_dir, resume_checkpoint)\n    print(f\"🔁 Resuming from checkpoint: {ckpt_path}\")\n    model = AutoModelForQuestionAnswering.from_pretrained(ckpt_path).to(device)\n    tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n\n# ======= Preprocessing =======\ndef preprocess_function(examples):\n    tok = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=stride,\n        padding=\"max_length\",\n        return_offsets_mapping=True,\n        return_overflowing_tokens=True\n    )\n    sample_map = tok.pop(\"overflow_to_sample_mapping\")\n    offset_map = tok.pop(\"offset_mapping\")\n    starts, ends = [], []\n\n    for i, offsets in enumerate(offset_map):\n        ids = tok[\"input_ids\"][i]\n        cls_index = ids.index(tokenizer.cls_token_id)\n        seq_ids = tok.sequence_ids(i)\n        sample_idx = sample_map[i]\n        answers = examples[\"answers\"][sample_idx]\n\n        if not answers[\"answer_start\"]:\n            starts.append(cls_index)\n            ends.append(cls_index)\n            continue\n\n        s_char = answers[\"answer_start\"][0]\n        text = answers[\"text\"][0]\n        e_char = s_char + len(text)\n\n        ts = next(j for j, sid in enumerate(seq_ids) if sid == 1)\n        te = len(ids) - 1\n        while seq_ids[te] != 1:\n            te -= 1\n\n        if not (offsets[ts][0] <= s_char and offsets[te][1] >= e_char):\n            starts.append(cls_index)\n            ends.append(cls_index)\n        else:\n            while ts < len(offsets) and offsets[ts][0] <= s_char:\n                ts += 1\n            starts.append(ts - 1)\n            while offsets[te][1] >= e_char:\n                te -= 1\n            ends.append(te + 1)\n\n    tok[\"start_positions\"] = starts\n    tok[\"end_positions\"] = ends\n    return tok\n\ntokenized_train = train_dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=train_dataset.column_names,\n    num_proc=num_proc\n)\n\ntokenized_valid = valid_dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=valid_dataset.column_names,\n    num_proc=num_proc\n)\n\ndef collate_fn(batch):\n    return {k: torch.tensor([d[k] for d in batch]) for k in batch[0]}\n\ndef save_checkpoint(model, tokenizer, epoch):\n    ckpt_path = os.path.join(checkpoint_dir, f\"checkpoint-epoch{epoch+1}\")\n    os.makedirs(ckpt_path, exist_ok=True)\n    mdl = model.module if hasattr(model, \"module\") else model\n    mdl.save_pretrained(ckpt_path)\n    tokenizer.save_pretrained(ckpt_path)\n    print(f\"💾 Saved checkpoint → {ckpt_path}\")\n\ntrain_loader = DataLoader(\n    tokenized_train,\n    batch_size=train_batch_size,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=2\n)\nvalid_loader = DataLoader(\n    tokenized_valid,\n    batch_size=eval_batch_size,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=2\n)\n\n# ======= Optimizer & Scheduler =======\noptimizer = AdamW(\n    model.parameters(),\n    lr=lr,\n    betas=(0.9, 0.999),\n    eps=1e-8\n)\n\nstart_epoch = 0\nif resume_checkpoint:\n    match = re.search(r\"epoch(\\d+)\", resume_checkpoint)\n    if match:\n        start_epoch = int(match.group(1))\n\neffective_epochs = epochs - start_epoch\ntotal_steps = len(train_loader) * effective_epochs\n\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps\n)\n\nscaler = GradScaler()\ntrain_losses, val_losses = [], []\nbest_val = float(\"inf\")\nno_improve = 0\npbar = tqdm(total=total_steps, desc=\"Training\")\n\n# ======= Training Loop =======\nfor epoch in range(start_epoch, epochs):\n    model.train()\n    running_loss = 0.0\n    optimizer.zero_grad()\n\n    for step, batch in enumerate(train_loader):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with autocast():\n            out = model(**batch)\n            loss = out.loss\n            if loss.dim() > 0:\n                loss = loss.mean()\n            loss = loss / accumulation_steps\n        scaler.scale(loss).backward()\n\n        if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_loader):\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n        running_loss += loss.item()\n        pbar.update(1)\n\n    avg_tr = running_loss / len(train_loader)\n    train_losses.append(avg_tr)\n    save_checkpoint(model, tokenizer, epoch)\n\n    # ======= Validation =======\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in valid_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            with autocast():\n                tmp = model(**batch).loss\n                if tmp.dim() > 0:\n                    tmp = tmp.mean()\n                val_loss += tmp.item()\n\n    avg_val = val_loss / len(valid_loader)\n    val_losses.append(avg_val)\n    print(f\"Epoch {epoch+1}/{epochs} → Train Loss: {avg_tr:.4f} | Val Loss: {avg_val:.4f}\")\n\n    mdl = model.module if hasattr(model, \"module\") else model\n    if avg_val < best_val:\n        best_val = avg_val\n        no_improve = 0\n        mdl.save_pretrained(model_save_dir)\n        tokenizer.save_pretrained(model_save_dir)\n        print(\"✅ Saved best model so far.\")\n    else:\n        no_improve += 1\n        print(f\"📉 No improvement for {no_improve} epoch(s).\")\n        if no_improve >= early_stopping_patience:\n            print(\"🛑 Early stopping.\")\n            break\n\nprint(\"✅ Done. Artifacts saved:\")\nprint(f\"  • Model/tokenizer → {model_save_dir}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======= Evaluation (EM / P / R / F1 / BLEU) =======\n# PHIÊN BẢN 1:\n!pip install nltk\nfrom collections import Counter\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n# ======= Evaluation (EM / P / R / F1 / BLEU) =======\ndef compute_exact(p, t):\n    return int(p.strip().lower() == t.strip().lower())\n\ndef compute_f1_precision_recall(p, t):\n    pt, tt = p.lower().split(), t.lower().split()\n    common = Counter(pt) & Counter(tt)\n    n = sum(common.values())\n    if n == 0:\n        return 0.0, 0.0, 0.0\n    prec = n / len(pt)\n    rec  = n / len(tt)\n    f1   = 2 * prec * rec / (prec + rec)\n    return f1, prec, rec\n    \n# Evaluation function\ndef evaluate(model, tokenizer, dataloader, dataset, max_samples=1000):\n    model.eval()\n    truths   = [itm[\"answers\"][\"text\"][0] if itm[\"answers\"][\"text\"] else \"\" for itm in dataset]\n    EMs, Ps, Rs, F1s = [], [], [], []\n    BLEUs = []\n    count = 0\n\n    for batch in dataloader:\n        if count >= max_samples:\n            break\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            out    = model(**batch)\n        starts = torch.argmax(out.start_logits, dim=-1)\n        ends   = torch.argmax(out.end_logits,   dim=-1)\n\n        for i, (ids, s, e) in enumerate(zip(batch[\"input_ids\"].cpu(), starts, ends)):\n            if count >= max_samples:\n                break\n            pred = tokenizer.decode(ids[s:e+1], skip_special_tokens=True).strip()\n            true = truths[count].strip()\n            count += 1\n            if not pred or not true:\n                continue\n            EMs.append(compute_exact(pred, true))\n            f1, p, r = compute_f1_precision_recall(pred, true)\n            F1s.append(f1); Ps.append(p); Rs.append(r)\n            BLEUs.append(sentence_bleu([true.split()], pred.split(), smoothing_function=SmoothingFunction().method1))\n\n    em   = np.mean(EMs) * 100 if EMs else 0.0\n    prec = np.mean(Ps)  * 100 if Ps  else 0.0\n    rec  = np.mean(Rs)  * 100 if Rs  else 0.0\n    f1   = np.mean(F1s) * 100 if F1s else 0.0\n    bleu = np.mean(BLEUs) * 100 if BLEUs else 0.0\n\n    print(\"\\n📊 Evaluation Results (in %):\")\n    print(f\"Exact Match: {em:.2f}%\")\n    print(f\"Precision:   {prec:.2f}%\")\n    print(f\"Recall:      {rec:.2f}%\")\n    print(f\"F1 Score:    {f1:.2f}%\")\n    print(f\"BLEU Score:  {bleu:.2f}%\")\n\n\n# Load best model and evaluate\nbest_model = AutoModelForQuestionAnswering.from_pretrained(model_save_dir).to(device)\nif torch.cuda.device_count() > 1:\n    best_model = nn.DataParallel(best_model)\n\nevaluate(best_model, tokenizer, valid_loader, valid_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======= Evaluation (EM / P / R / F1 / BLEU) =======\n# PHIÊN BẢN 2: thêm vài hàm chuẩn hóa\n# kết quả tốt hơn 1\ndef normalize_and_tokenize(text):\n    return re.findall(r\"\\w+|[^\\w\\s]\", text.lower())\n\ndef compute_exact(p, t):\n    if not p or not t:\n        return 0\n    return int(p.strip().lower() == t.strip().lower())\n\ndef compute_f1_precision_recall(p, t):\n    if not p or not t:\n        return 0.0, 0.0, 0.0\n\n    ptokens = normalize_and_tokenize(p)\n    ttokens = normalize_and_tokenize(t)\n\n    common = Counter(ptokens) & Counter(ttokens)\n    n_common = sum(common.values())\n    if n_common == 0:\n        return 0.0, 0.0, 0.0\n\n    prec = n_common / len(ptokens)\n    rec  = n_common / len(ttokens)\n    f1   = 2 * prec * rec / (prec + rec)\n    return f1, prec, rec\ndef evaluate(model, tokenizer, dataloader, dataset, max_samples=1000):\n    model.eval()\n\n    truths = [itm[\"answers\"][\"text\"][0] if itm[\"answers\"][\"text\"] else \"\" for itm in dataset]\n    EMs, Ps, Rs, F1s, BLEUs = [], [], [], [], []\n    count = 0\n\n    for batch in dataloader:\n        if count >= max_samples:\n            break\n        batch = {k: v.to(device) for k,v in batch.items()}\n        with torch.no_grad():\n            out = model(**batch)\n        starts = torch.argmax(out.start_logits, dim=-1).cpu()\n        ends   = torch.argmax(out.end_logits,   dim=-1).cpu()\n\n        for ids, s, e in zip(batch[\"input_ids\"].cpu(), starts, ends):\n            if count >= max_samples:\n                break\n            true = truths[count].strip()\n            pred = tokenizer.decode(ids[s:e+1], skip_special_tokens=True).strip()\n            if not true:\n                count += 1\n                continue\n\n            EMs.append(compute_exact(pred, true))\n            f1, p, r = compute_f1_precision_recall(pred, true)\n            F1s.append(f1); Ps.append(p); Rs.append(r)\n            BLEUs.append(sentence_bleu(\n                [true.split()], pred.split(),\n                smoothing_function=SmoothingFunction().method1\n            ))\n\n            count += 1\n\n    print(f\"Exact Match: {np.mean(EMs):.4f}\")\n    print(f\"Precision:   {np.mean(Ps):.4f}\")\n    print(f\"Recall:      {np.mean(Rs):.4f}\")\n    print(f\"F1 Score:    {np.mean(F1s):.4f}\")\n    print(f\"BLEU Score:  {np.mean(BLEUs):.4f}\")\n# Load best model and evaluate\nbest_model = AutoModelForQuestionAnswering.from_pretrained(model_save_dir).to(device)\nif torch.cuda.device_count() > 1:\n    best_model = nn.DataParallel(best_model)\n\nevaluate(best_model, tokenizer, valid_loader, valid_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======= Evaluation (EM / P / R / F1 / BLEU) =======\n# PHIÊN BẢN 2\n#Kết quả tốt hơn 1\nfrom collections import Counter\nimport numpy as np\nimport re\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\ndef normalize_and_tokenize(text):\n    return re.findall(r\"\\w+|[^\\w\\s]\", text.lower())\n\ndef compute_metrics_single(pred, truths):\n    if not pred and all(not t for t in truths):\n        return 1, 1.0, 1.0, 1.0, 1.0\n    ems = [int(pred.strip().lower() == t.strip().lower()) for t in truths]\n    EM = max(ems)\n\n    # Tokenize once\n    ptoks = normalize_and_tokenize(pred)\n    best = (0.0, 0.0, 0.0) \n    for t in truths:\n        ttoks = normalize_and_tokenize(t)\n        common = Counter(ptoks) & Counter(ttoks)\n        n_common = sum(common.values())\n        if n_common == 0:\n            continue\n        prec = n_common / len(ptoks)\n        rec  = n_common / len(ttoks)\n        f1   = 2 * prec * rec / (prec + rec)\n        if f1 > best[0]:\n            best = (f1, prec, rec)\n    F1, P, R = best\n\n    # BLEU-4 với smoothing\n    list_of_references = [normalize_and_tokenize(t) for t in truths]\n    # Nếu không có token nào chung sẽ trả về 0, smoothing cải thiện\n    BLEU = sentence_bleu(\n        list_of_references,\n        ptoks,\n        weights=(0.25, 0.25, 0.25, 0.25),\n        smoothing_function=SmoothingFunction().method2\n    )\n\n    return EM, F1, P, R, BLEU\n\ndef evaluate(model, tokenizer, dataloader, dataset, max_samples=1000):\n    model.eval()\n    truths_all = [itm[\"answers\"][\"text\"] for itm in dataset]\n    EMs, F1s, Ps, Rs, BLEUs = [], [], [], [], []\n    count = 0\n\n    for batch in dataloader:\n        if count >= max_samples:\n            break\n        batch = {k: v.to(device) for k,v in batch.items()}\n        with torch.no_grad():\n            out = model(**batch)\n        starts = torch.argmax(out.start_logits, dim=-1).cpu()\n        ends   = torch.argmax(out.end_logits,   dim=-1).cpu()\n\n        for ids, s, e in zip(batch[\"input_ids\"].cpu(), starts, ends):\n            if count >= max_samples:\n                break\n\n            pred = tokenizer.decode(ids[s:e+1], skip_special_tokens=True).strip()\n            truths = truths_all[count]\n            # Bỏ qua hoàn toàn khi không có bất kỳ truth nào?\n            if not truths:\n                count += 1\n                continue\n\n            EM, f1, p, r, bleu = compute_metrics_single(pred, truths)\n            EMs.append(EM)\n            F1s.append(f1); Ps.append(p); Rs.append(r); BLEUs.append(bleu)\n            count += 1\n\n    print(f\"Exact Match: {np.mean(EMs):.4f}\")\n    print(f\"Precision:   {np.mean(Ps):.4f}\")\n    print(f\"Recall:      {np.mean(Rs):.4f}\")\n    print(f\"F1 Score:    {np.mean(F1s):.4f}\")\n    print(f\"BLEU-4:      {np.mean(BLEUs):.4f}\")\n\n\n# ————————————————————————————————————————————————\n# Ví dụ gọi hàm:\nbest_model = AutoModelForQuestionAnswering.from_pretrained(model_save_dir).to(device)\nif torch.cuda.device_count() > 1:\n    best_model = torch.nn.DataParallel(best_model)\ntokenizer   = AutoTokenizer.from_pretrained(model_save_dir)\nevaluate(best_model, tokenizer, valid_loader, valid_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T03:02:35.640017Z","iopub.execute_input":"2025-05-11T03:02:35.640313Z","execution_failed":"2025-05-11T03:02:35.795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Trả lời thử với 10 câu hỏi \nfrom transformers import AutoTokenizer, RobertaForQuestionAnswering\n\n# Load model and tokenizer\nmodel_dir = \"/kaggle/working/best_model\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = RobertaForQuestionAnswering.from_pretrained(model_dir)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\n# 🧠 Advanced QA function\ndef answer_question_advanced(question, context):\n    inputs = tokenizer(\n        question,\n        context,\n        return_tensors=\"pt\",\n        truncation=\"only_second\",\n        max_length=384,\n        stride=128,\n        padding=\"max_length\",\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        return_token_type_ids=True\n    ).to(device)\n\n    input_ids = inputs[\"input_ids\"]\n    attention_mask = inputs[\"attention_mask\"]\n    offset_mapping = inputs[\"offset_mapping\"]\n    token_type_ids = inputs[\"token_type_ids\"]\n\n    best_score = -float(\"inf\")\n    best_answer = \"\"\n    best_start, best_end = None, None\n    best_start_word, best_end_word = \"\", \"\"\n\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        start_logits = outputs.start_logits\n        end_logits = outputs.end_logits\n\n    for i in range(len(input_ids)):\n        start_logit = start_logits[i]\n        end_logit = end_logits[i]\n\n        start_idx = torch.argmax(start_logit).item()\n        end_idx = torch.argmax(end_logit).item()\n\n        if start_idx > end_idx:\n            continue\n\n        score = start_logit[start_idx] + end_logit[end_idx]\n\n        if score > best_score:\n            best_score = score\n            offsets = offset_mapping[i].cpu().tolist()\n            input_id = input_ids[i].cpu().tolist()\n\n            start_char = offsets[start_idx][0]\n            end_char = offsets[end_idx][1]\n\n            if start_char is None or end_char is None:\n                continue\n\n            best_answer = context[start_char:end_char]\n            best_start = start_idx\n            best_end = end_idx\n            best_start_word = tokenizer.convert_ids_to_tokens([input_id[start_idx]])[0]\n            best_end_word = tokenizer.convert_ids_to_tokens([input_id[end_idx]])[0]\n\n    if best_answer.strip() == \"\":\n        return {\n            \"start_token\": None,\n            \"end_token\": None,\n            \"start_word\": \"\",\n            \"end_word\": \"\",\n            \"answer\": \"🤔 No suitable answer found.\"\n        }\n\n    return {\n        \"start_token\": best_start,\n        \"end_token\": best_end,\n        \"start_word\": best_start_word,\n        \"end_word\": best_end_word,\n        \"answer\": best_answer.strip()\n    }\n\n# 🧪 Test cases with extended context\ntest_cases = [\n    {\n        \"question\": \"Who developed BERT?\",\n        \"context\": (\n            \"BERT, which stands for Bidirectional Encoder Representations from Transformers, is a natural language \"\n            \"processing model introduced by Google in 2018. The model was developed by researchers Jacob Devlin, \"\n            \"Ming-Wei Chang, Kenton Lee, and Kristina Toutanova at Google AI Language. BERT marked a major advancement \"\n            \"in the use of transformers for NLP tasks and inspired a wide range of follow-up research and variants such as RoBERTa, DistilBERT, and ALBERT.\"\n        )\n    },\n    {\n        \"question\": \"When was BERT introduced?\",\n        \"context\": (\n            \"The BERT model was officially introduced to the machine learning community in 2018 through a research paper \"\n            \"titled 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'. It was presented \"\n            \"by researchers at Google AI. The paper detailed a new method for pre-training language representations that \"\n            \"significantly improved the performance of a wide range of NLP tasks.\"\n        )\n    },\n    {\n        \"question\": \"What does BERT stand for?\",\n        \"context\": (\n            \"BERT is an acronym for Bidirectional Encoder Representations from Transformers. It represents a new approach \"\n            \"in natural language processing where deep bidirectional representations are pre-trained by jointly conditioning \"\n            \"on both left and right context in all layers. This contrasts with earlier models that used unidirectional training, \"\n            \"which limited their ability to understand full context.\"\n        )\n    },\n    {\n        \"question\": \"What tasks is BERT used for?\",\n        \"context\": (\n            \"BERT has been widely adopted for a variety of natural language processing tasks. These include question answering, \"\n            \"sentence classification, sentiment analysis, named entity recognition, and language inference. One of BERT’s key strengths \"\n            \"is its ability to be fine-tuned on small task-specific datasets while still achieving state-of-the-art performance. \"\n            \"This flexibility has led to its widespread use in both academic and commercial applications.\"\n        )\n    },\n    {\n        \"question\": \"What is the innovation behind BERT?\",\n        \"context\": (\n            \"The core innovation behind BERT lies in its use of a masked language model (MLM) objective and next sentence prediction (NSP) \"\n            \"during pre-training, allowing it to deeply understand the context of words. Unlike previous models that trained language models \"\n            \"in a left-to-right or right-to-left fashion, BERT is trained bidirectionally. This means it considers both left and right context \"\n            \"simultaneously when learning word representations. This approach significantly improves comprehension and has advanced the state of the art \"\n            \"on multiple NLP benchmarks such as SQuAD and GLUE.\"\n        )\n    },\n    {\n        \"question\": \"What is the function of red blood cells?\",\n        \"context\": (\n            \"Red blood cells, also known as erythrocytes, are a crucial component of human blood. Their primary function is to \"\n            \"transport oxygen from the lungs to the rest of the body and return carbon dioxide from the tissues back to the lungs. \"\n            \"This is made possible by hemoglobin, a protein within red blood cells that binds to oxygen molecules. An adequate number \"\n            \"of red blood cells is essential for maintaining normal bodily functions and preventing conditions such as anemia.\"\n        )\n    },\n    {\n        \"question\": \"What causes high blood pressure?\",\n        \"context\": (\n            \"High blood pressure, or hypertension, occurs when the force of blood pushing against the walls of the arteries is consistently too high. \"\n            \"It can be caused by a variety of factors including poor diet (high in salt and saturated fats), lack of physical activity, obesity, stress, \"\n            \"genetics, and certain chronic conditions such as kidney disease. If left uncontrolled, high blood pressure can lead to serious complications \"\n            \"like heart disease, stroke, and kidney failure.\"\n        )\n    },\n    {\n        \"question\": \"How do vaccines work?\",\n        \"context\": (\n            \"Vaccines work by training the immune system to recognize and fight specific pathogens, such as viruses or bacteria. When a person receives a vaccine, \"\n            \"their body is exposed to a harmless form of the pathogen—often a weakened or inactivated version, or just a piece of it like a protein. \"\n            \"This exposure triggers an immune response, allowing the body to create memory cells that will recognize and combat the real pathogen if it is encountered \"\n            \"in the future. This mechanism is the foundation of immunization and helps prevent the spread of infectious diseases.\"\n        )\n    },\n    {\n        \"question\": \"What is diabetes?\",\n        \"context\": (\n            \"Diabetes is a chronic medical condition that occurs when the body is unable to properly regulate blood sugar (glucose) levels. \"\n            \"There are two main types of diabetes: Type 1, where the body does not produce insulin, and Type 2, where the body does not use insulin effectively. \"\n            \"Insulin is a hormone that helps glucose enter the cells to be used for energy. Without proper insulin function, glucose builds up in the bloodstream, \"\n            \"leading to various health problems such as cardiovascular disease, nerve damage, and kidney failure if left unmanaged.\"\n        )\n    },\n    {\n        \"question\": \"What are the symptoms of COVID-19?\",\n        \"context\": (\n            \"COVID-19, caused by the SARS-CoV-2 virus, can present with a wide range of symptoms. Common symptoms include fever, cough, fatigue, loss of taste or smell, \"\n            \"and difficulty breathing. Some individuals may also experience muscle aches, sore throat, diarrhea, and headaches. In severe cases, COVID-19 can lead to pneumonia, \"\n            \"acute respiratory distress, and death, particularly among older adults and those with underlying health conditions. Asymptomatic cases are also possible, \"\n            \"contributing to the rapid spread of the virus.\"\n        )\n    }\n]\n\n\n# 🔍 Run and display results\nfor idx, case in enumerate(test_cases, 1):\n    print(f\"\\n🔹 Test Case {idx}\")\n    print(f\"❓ Question: {case['question']}\")\n    print(f\"📘 Context: {case['context']}\")\n    result = answer_question_advanced(case[\"question\"], case[\"context\"])\n    print(f\"✅ Answer: {result['answer']}\")\n    print(f\"🔢 Start Token Index: {result['start_token']} ({result['start_word'].replace('Ġ', '')})\")\n    print(f\"🔢 End Token Index: {result['end_token']} ({result['end_word'].replace('Ġ', '')})\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}