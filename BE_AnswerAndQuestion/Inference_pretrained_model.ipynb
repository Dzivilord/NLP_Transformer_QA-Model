{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11704580,"sourceType":"datasetVersion","datasetId":7346783}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\nfrom torch.cuda.amp import autocast\nfrom collections import Counter\nimport re\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nvalid_path = '/kaggle/input/cleaned/valid_dataset.json'\nmodel_name = \"deepset/roberta-large-squad2\"\nbatch_size = 16\nmax_length = 150\nstride     = 15\ndevice     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# â”€â”€â”€ Load dataset & tokenizer/model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nraw_valid = load_dataset(\"json\", data_files={\"validation\": valid_path})[\"validation\"]\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel     = AutoModelForQuestionAnswering.from_pretrained(model_name).to(device)\n\n# Multi-GPU\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\n\nmodel.eval()  # gá»i sau khi bá»c DataParallel\n\n# â”€â”€â”€ Preprocessing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef preprocess_function(examples):\n    tok = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=stride,\n        padding=\"max_length\",\n        return_offsets_mapping=True,\n        return_overflowing_tokens=True\n    )\n    sample_map = tok.pop(\"overflow_to_sample_mapping\")\n    offset_map = tok.pop(\"offset_mapping\")\n\n    starts, ends = [], []\n    for i, offsets in enumerate(offset_map):\n        ids = tok[\"input_ids\"][i]\n        cls_index = ids.index(tokenizer.cls_token_id)\n        seq_ids = tok.sequence_ids(i)\n        sample_idx = sample_map[i]\n        answers = examples[\"answers\"][sample_idx]\n\n        if not answers[\"answer_start\"]:\n            starts.append(cls_index); ends.append(cls_index); continue\n\n        s_char = answers[\"answer_start\"][0]\n        text   = answers[\"text\"][0]\n        e_char = s_char + len(text)\n\n        ts = next(j for j, sid in enumerate(seq_ids) if sid == 1)\n        te = len(ids) - 1\n        while seq_ids[te] != 1: te -= 1\n\n        if not (offsets[ts][0] <= s_char and offsets[te][1] >= e_char):\n            starts.append(cls_index); ends.append(cls_index)\n        else:\n            while ts < len(offsets) and offsets[ts][0] <= s_char: ts += 1\n            starts.append(ts - 1)\n            while te >= 0 and offsets[te][1] >= e_char: te -= 1\n            ends.append(te + 1)\n\n    tok[\"start_positions\"] = starts\n    tok[\"end_positions\"]   = ends\n    tok[\"offset_mapping\"]                = offset_map\n    tok[\"overflow_to_sample_mapping\"]   = sample_map\n    return tok\n\ntokenized_valid = raw_valid.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=raw_valid.column_names,\n    num_proc=4\n)\n\n# â”€â”€â”€ DataLoader â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef collate_fn(batch):\n    tensor_keys = [\"input_ids\",\"attention_mask\",\"start_positions\",\"end_positions\"]\n    collated = {k: torch.tensor([d[k] for d in batch]) for k in tensor_keys}\n    collated[\"offset_mapping\"]             = [d[\"offset_mapping\"] for d in batch]\n    collated[\"overflow_to_sample_mapping\"] = [d[\"overflow_to_sample_mapping\"] for d in batch]\n    return collated\n\nvalid_loader = DataLoader(\n    tokenized_valid, batch_size=batch_size,\n    shuffle=False, collate_fn=collate_fn, num_workers=0\n)\n\n# â”€â”€â”€ Metric helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef normalize_and_tokenize(text):\n    return re.findall(r\"\\w+|[^\\w\\s]\", text.lower())\n\ndef compute_metrics_single(pred, truths):\n    ems = [int(pred.strip().lower() == t.strip().lower()) for t in truths]\n    EM = max(ems) if truths else 1\n    ptoks = normalize_and_tokenize(pred)\n    best_f1 = best_prec = best_rec = 0.0\n    for t in truths:\n        ttoks = normalize_and_tokenize(t)\n        common = Counter(ptoks) & Counter(ttoks)\n        n_common = sum(common.values())\n        if n_common == 0:\n            continue\n        prec = n_common / len(ptoks) if ptoks else 0\n        rec  = n_common / len(ttoks) if ttoks else 0\n        f1   = 2 * prec * rec / (prec + rec) if prec + rec > 0 else 0\n        if f1 > best_f1:\n            best_f1, best_prec, best_rec = f1, prec, rec\n    refs = [normalize_and_tokenize(t) for t in truths]\n    BLEU = sentence_bleu(refs, ptoks, weights=(0.25,0.25,0.25,0.25),\n                         smoothing_function=SmoothingFunction().method2)\n    return EM, best_f1, best_prec, best_rec, BLEU\n\n# â”€â”€â”€ Inference & Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nall_EM, all_F1, all_P, all_R, all_B = [], [], [], [], []\n\nwith torch.no_grad():\n    for batch in tqdm(valid_loader):\n        input_ids      = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        with autocast():\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\n        start_logits = outputs.start_logits.cpu().numpy()\n        end_logits   = outputs.end_logits.cpu().numpy()\n\n        for i in range(input_ids.size(0)):\n            sample_idx = batch[\"overflow_to_sample_mapping\"][i]\n            raw = raw_valid[sample_idx]\n            context = raw[\"context\"]\n            truths = raw[\"answers\"][\"text\"]\n\n            s_idx = np.argmax(start_logits[i])\n            e_idx = np.argmax(end_logits[i])\n            offsets = batch[\"offset_mapping\"][i]\n\n            if s_idx >= len(offsets) or e_idx >= len(offsets) or s_idx > e_idx:\n                pred = \"\"\n            else:\n                s_char, e_char = offsets[s_idx][0], offsets[e_idx][1]\n                pred = context[s_char:e_char].strip()\n\n            EM, F1, P, R, BLEU = compute_metrics_single(pred, truths)\n            all_EM.append(EM); all_F1.append(F1)\n            all_P.append(P); all_R.append(R); all_B.append(BLEU)\n\n# â”€â”€â”€ Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\nğŸ“Š QA Metrics:\")\nprint(f\"Exact Match : {np.mean(all_EM)*100:5.2f}%\")\nprint(f\"F1 Score    : {np.mean(all_F1)*100:5.2f}%\")\nprint(f\"Precision   : {np.mean(all_P)*100:5.2f}%\")\nprint(f\"Recall      : {np.mean(all_R)*100:5.2f}%\")\nprint(f\"BLEU-4      : {np.mean(all_B)*100:5.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T07:30:41.014703Z","iopub.execute_input":"2025-05-11T07:30:41.014953Z","iopub.status.idle":"2025-05-11T08:13:34.443699Z","shell.execute_reply.started":"2025-05-11T07:30:41.014933Z","shell.execute_reply":"2025-05-11T08:13:34.442988Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e1b02b7240a423e82cc69f66d79b066"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"648396b11bc54704904d1314a07cc466"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/696 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ba7910609654b8892c03cad836e46dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62555886dc4e40f3b43a7664ff067f80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7610bb6c04d64344a83d1c08a5728a47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8561a148ce124b7f9cb06baabc62dbae"}},"metadata":{}},{"name":"stderr","text":"2025-05-11 07:31:08.087372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746948668.473425      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746948668.588768      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f66cfaa70f2434cb49c444588cbfac2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/32739 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc75e9da79924fe093ee86f5dde70e96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/11734 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a4a015435184f7dae6b9a66700123e1"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/1150701214.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ“Š QA Metrics:\nExact Match :  0.00%\nF1 Score    :  0.26%\nPrecision   :  0.38%\nRecall      :  0.45%\nBLEU-4      :  0.13%\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"print(\"\\nğŸ“Š QA Metrics:\")\nprint(f\"Exact Match : {np.mean(all_EM):5.4f}%\")\nprint(f\"F1 Score    : {np.mean(all_F1):5.4f}%\")\nprint(f\"Precision   : {np.mean(all_P):5.4f}%\")\nprint(f\"Recall      : {np.mean(all_R):5.4f}%\")\nprint(f\"BLEU-4      : {np.mean(all_B):5.4f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T08:17:16.469196Z","iopub.execute_input":"2025-05-11T08:17:16.469456Z","iopub.status.idle":"2025-05-11T08:17:16.537612Z","shell.execute_reply.started":"2025-05-11T08:17:16.469438Z","shell.execute_reply":"2025-05-11T08:17:16.536973Z"}},"outputs":[{"name":"stdout","text":"\nğŸ“Š QA Metrics:\nExact Match : 0.0000%\nF1 Score    : 0.0026%\nPrecision   : 0.0038%\nRecall      : 0.0045%\nBLEU-4      : 0.0013%\n","output_type":"stream"}],"execution_count":3}]}